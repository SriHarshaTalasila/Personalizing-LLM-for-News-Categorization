{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f96dc1-ae78-42a1-a3d5-a3c2179bdb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ----------------- --------------------- 61.4/134.8 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 134.8/134.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp39-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.8 MB 1.7 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.1/8.8 MB 1.4 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/8.8 MB 1.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/8.8 MB 1.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/8.8 MB 1.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/8.8 MB 1.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/8.8 MB 1.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/8.8 MB 1.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.6/8.8 MB 1.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.7/8.8 MB 1.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.8 MB 1.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/8.8 MB 1.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.1/8.8 MB 1.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.1/8.8 MB 1.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.3/8.8 MB 1.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.3/8.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.4/8.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.5/8.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.7/8.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.7/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.9/8.8 MB 1.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.9/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.2/8.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.3/8.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/8.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.5/8.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.7/8.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.7/8.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.9/8.8 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.0/8.8 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.2/8.8 MB 2.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.3/8.8 MB 2.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.5/8.8 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.6/8.8 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.8/8.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.0/8.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.1/8.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.3/8.8 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.4/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.9/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/8.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.2/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.3/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.5/8.8 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.8/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.0/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.1/8.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.3/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.0/8.8 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.2/8.8 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/8.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.8/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.0/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.8 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.6 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 286.7/388.6 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.6/388.6 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp39-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.7 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 194.6/269.7 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.7/269.7 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 7.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.22.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SentencePiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 41.0/991.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 71.7/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---- --------------------------------- 112.6/991.5 kB 939.4 kB/s eta 0:00:01\n",
      "   ----- -------------------------------- 143.4/991.5 kB 853.3 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 204.8/991.5 kB 958.4 kB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 276.5/991.5 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 358.4/991.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 409.6/991.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 491.5/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 563.2/991.5 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 645.1/991.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 757.8/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 860.2/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 952.3/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdea603f-cab5-4baa-94dd-0136b5ea648a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "#input_text = \"translate English to German: How old are you?\"\n",
    "#input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#outputs = model.generate(input_ids)\n",
    "#print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401c1236-3c1c-4207-acab-0dc4846fef28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72b52289-e619-46cb-8d6e-b5f0cc79e704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json('/content/drive/MyDrive/Information_retrieval/Datasets/train_Input_UserBased_PNC.json')\n",
    "FILE_PATH = \"Datasets/validation_Input_UserBased_PNC.json\"\n",
    "df = pd.read_json(FILE_PATH)\n",
    "#articles = pd.DataFrame(df['input'].apply(lambda x: x.split(\"article:\", 1)[-1].strip() if \"article:\" in x else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f634c730-526a-4858-b265-566514d42d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Which category does this article relate to amo...\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"input\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#candidate_labels = ['women', 'religion', 'politics', 'style & beauty', 'entertainment', 'culture & arts', 'sports', 'science & technology', 'travel', 'business', 'crime', 'education', 'healthy living', 'parents', 'food & drink']\n",
    "pred_list = []\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    print(\"Predicting for id: \", row[\"id\"])\n",
    "    input_text = row[\"input\"]\n",
    "    #print(input_text)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    outputs = model.generate(input_ids)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    #print(type(answer))\n",
    "    #print(answer)\n",
    "    # Find the label with the highest score\n",
    "    #max_label = max(zip(answer['labels'], answer['scores']), key=lambda x: x[1])[0]\n",
    "    \n",
    "    data = {}\n",
    "    data[\"id\"] = str(row[\"id\"])\n",
    "    data[\"output\"] = answer\n",
    "    pred_list.append(data)\n",
    "\n",
    "final_json = { \"task\" : \"LaMP_2\", \"golds\" : pred_list }\n",
    "print(len(pred_list), \" items added in the data_list\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f212aa7f-afc8-4035-b582-811a15e416ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the JSON data to a file in the local directory\n",
    "json_dumped_file = json.dumps(final_json, indent=2)\n",
    "\n",
    "json_file_path = \"baseline_validation_Input_UserBased_PNC.json\"\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json_file.write(json_dumped_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c8bad07-a49b-44bd-adfb-daacca1d1684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n"
     ]
    }
   ],
   "source": [
    "sequence_to_classify = \"It's hard to find a restaurant that doesn't now place a little card at your table inquiring if the establishment was: (a) really awful; (b) tolerable; (c) sublime.\"\n",
    "candidate_labels = ['women', 'religion', 'politics', 'style & beauty', 'entertainment', 'culture & arts', 'sports', 'science & technology', 'travel', 'business', 'crime', 'education', 'healthy living', 'parents', 'food & drink']\n",
    "answer = classifier(sequence_to_classify, candidate_labels)\n",
    "#{'labels': ['travel', 'dancing', 'cooking'],\n",
    "# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n",
    "# 'sequence': 'one day I will see the world'}\n",
    "\n",
    "# Find the label with the highest score\n",
    "max_label = max(zip(answer['labels'], answer['scores']), key=lambda x: x[1])[0]\n",
    "\n",
    "print(max_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b930b7a-64fc-4ee8-b8ee-5c3ad247e22b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(pred_list)\n",
    "# string_json_data = json.dumps(final_json, indent=2)\n",
    "# # Save the JSON data to a file in the local directory\n",
    "# json_file_path = \"validation_Output_UserBased_PNC.json\"\n",
    "# with open(json_file_path, \"w\") as json_file:\n",
    "#     json_file.write(string_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424ee2c-154e-4ee9-a1e0-ff5d2a03b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to JSON format\n",
    "#json_data = json.dumps(data_list, indent=2)\n",
    "\n",
    "# Save the JSON data to a file in the local directory\n",
    "json_file_path = \"personalized_train_Input_UserBased_PNC.json\"\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
